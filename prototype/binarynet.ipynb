{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn import datasets\n",
    "import mxnet as mx\n",
    "mnist = mx.test_utils.get_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = mnist['train_data']\n",
    "x = x.reshape(x.shape[0], -1)\n",
    "y = mnist['train_label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<input>:51: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:71: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:51: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:71: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:51: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:71: DeprecationWarning: invalid escape sequence \\s\n",
      "<ipython-input-24-5e39c44c1f8f>:51: DeprecationWarning: invalid escape sequence \\s\n",
      "  \"\"\"\n",
      "<ipython-input-24-5e39c44c1f8f>:71: DeprecationWarning: invalid escape sequence \\s\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Weights must be of the format (in, hidden)\n",
    "shapes = [784, 100, 10]\n",
    "W = [np.random.randn(shapes[i], shapes[i+1]) for i in range(len(shapes)-1)]\n",
    "\n",
    "def affine(x, w):\n",
    "    \"\"\"\n",
    "    x : (n, m)\n",
    "    w : (m, k)\n",
    "    out : (n, k)\n",
    "    \"\"\"\n",
    "    return x.dot(w)\n",
    "\n",
    "def affine_(d, x, w):\n",
    "    \"\"\"\n",
    "    x : (n, m)\n",
    "    w : (m, k)\n",
    "    d : (n, k)\n",
    "    ---\n",
    "    dw : (m, k)\n",
    "    dx : (n, m)\n",
    "    \"\"\"\n",
    "    return {'x': d.dot(w.T), 'w': x.T.dot(d)}\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    x : (n, m)\n",
    "    out : (n, m)\n",
    "    \"\"\"\n",
    "    return sp.special.expit(x)\n",
    "\n",
    "def sigmoid_(d, x):\n",
    "    \"\"\"\n",
    "    x : (n, m)\n",
    "    d : (n, m)\n",
    "    ---\n",
    "    dx : (n, m)\n",
    "    \"\"\"\n",
    "    sigm = sp.special.expit(x)\n",
    "    return d * (sigm * (1-sigm))\n",
    "\n",
    "def softmax_ce(x, y):\n",
    "    \"\"\"\n",
    "    x : (n, m)\n",
    "    y : (n,)\n",
    "    out : () [scalar]\n",
    "    \n",
    "    Equation is 1/n * \\sum_i^n [ -log(e^x_{y_i} / \\sum_j e^x_j) ]\n",
    "    which is equivalently:\n",
    "        \n",
    "        1/n * \\sum_i^n log(\\sum_j e^x_j) - x_{y_i}\n",
    "    \"\"\"\n",
    "    n = x.shape[0]\n",
    "    exp = np.exp(x)\n",
    "    # denominator in original expression, after log\n",
    "    denom = np.log(np.sum(exp, axis=1))\n",
    "    return np.sum(denom - x[np.arange(n), y]) / n\n",
    "\n",
    "def softmax_ce_(x, y):\n",
    "    \"\"\"\n",
    "    x : (n, m)\n",
    "    y : (n,)\n",
    "    ---\n",
    "    dx : (n, m)\n",
    "    \n",
    "    Back propagation for a single data point is:\n",
    "    \n",
    "    dL_i/dx_{ik} = -1_{k == y_i} + softmax(x_i)_k\n",
    "    \n",
    "    thus, for all data points:\n",
    "    dL/dx_k = 1/n * \\sum_i -1_{k == y_i} + softmax(x_i)_k\n",
    "    \"\"\"\n",
    "    n = x.shape[0]\n",
    "    exp = np.exp(x)\n",
    "    softmax = exp / np.expand_dims(np.sum(exp, axis=1), 1)\n",
    "    softmax[np.arange(n), y] -= 1\n",
    "    return softmax / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at 0 : 10.864425421043748\n",
      "loss at 1 : 10.217508834486656\n",
      "loss at 2 : 9.62265561508035\n",
      "loss at 3 : 9.430731911468166\n",
      "loss at 4 : 9.421496707046531\n",
      "loss at 5 : 8.82202434865556\n",
      "loss at 6 : 8.450189027497188\n",
      "loss at 7 : 8.001155364761852\n",
      "loss at 8 : 7.867413572632432\n",
      "loss at 9 : 7.362482122536241\n",
      "loss at 10 : 7.357904459091113\n",
      "loss at 11 : 7.156522088259499\n",
      "loss at 12 : 6.893528333088547\n",
      "loss at 13 : 6.504667295983222\n",
      "loss at 14 : 6.501438330690913\n",
      "loss at 15 : 6.4036233203365995\n",
      "loss at 16 : 6.412673175043907\n",
      "loss at 17 : 6.396620285737429\n",
      "loss at 18 : 6.307524135955853\n",
      "loss at 19 : 6.156097147056302\n",
      "loss at 20 : 5.834342843988371\n",
      "loss at 21 : 5.673409952134881\n",
      "loss at 22 : 5.647192175945023\n",
      "loss at 23 : 5.5132415178341585\n",
      "loss at 24 : 5.5093894779666535\n",
      "loss at 25 : 5.5209349851354235\n",
      "loss at 26 : 5.447772656561441\n",
      "loss at 27 : 5.381147785981573\n",
      "loss at 28 : 5.401137316252896\n",
      "loss at 29 : 5.3817608989869266\n",
      "loss at 30 : 5.2210104776675195\n",
      "loss at 31 : 5.205435073451499\n",
      "loss at 32 : 5.206867775754948\n",
      "loss at 33 : 5.0242145236459095\n",
      "loss at 34 : 4.836231898157139\n",
      "loss at 35 : 4.802616824420485\n",
      "loss at 36 : 4.832981888409138\n",
      "loss at 37 : 4.737820299803543\n",
      "loss at 38 : 4.7293036236832044\n",
      "loss at 39 : 4.8037786917582155\n",
      "loss at 40 : 4.625136044046758\n",
      "loss at 41 : 4.570187597799728\n",
      "loss at 42 : 4.583089268329314\n",
      "loss at 43 : 4.632184333143475\n",
      "loss at 44 : 4.561795873624061\n",
      "loss at 45 : 4.572555419574012\n",
      "loss at 46 : 4.503205205954178\n",
      "loss at 47 : 4.481187747261104\n",
      "loss at 48 : 4.483954453673027\n",
      "loss at 49 : 4.2984006716169505\n",
      "loss at 50 : 4.238360861130198\n",
      "loss at 51 : 4.254182007398867\n",
      "loss at 52 : 4.494011786947728\n",
      "loss at 53 : 4.536715258716751\n",
      "loss at 54 : 4.269223682513975\n",
      "loss at 55 : 4.138314074606728\n",
      "loss at 56 : 4.143147414926138\n",
      "loss at 57 : 4.108408955346411\n",
      "loss at 58 : 4.140255299863096\n",
      "loss at 59 : 4.136794020123454\n",
      "loss at 60 : 4.04578664498852\n",
      "loss at 61 : 3.985074695509809\n",
      "loss at 62 : 4.008958416132965\n",
      "loss at 63 : 3.994337122467381\n",
      "loss at 64 : 3.951426030173872\n",
      "loss at 65 : 3.931903691681066\n",
      "loss at 66 : 3.9885489632952336\n",
      "loss at 67 : 3.9833017455488724\n",
      "loss at 68 : 4.262146493387939\n",
      "loss at 69 : 3.8884806682550535\n",
      "loss at 70 : 3.8585705071088157\n",
      "loss at 71 : 3.8762142381956863\n",
      "loss at 72 : 3.909820603125813\n",
      "loss at 73 : 3.898454010208486\n",
      "loss at 74 : 3.8319096709402514\n",
      "loss at 75 : 3.7516071549481547\n",
      "loss at 76 : 3.867471454296878\n",
      "loss at 77 : 3.868671462147524\n",
      "loss at 78 : 3.8395288045291283\n",
      "loss at 79 : 3.7587029102760314\n",
      "loss at 80 : 3.892888276389315\n",
      "loss at 81 : 3.83887904046099\n",
      "loss at 82 : 3.8017140355406456\n",
      "loss at 83 : 3.7934229490099756\n",
      "loss at 84 : 3.88697626257433\n",
      "loss at 85 : 3.7172625767201724\n",
      "loss at 86 : 3.677493038272749\n",
      "loss at 87 : 3.8977927505076133\n",
      "loss at 88 : 3.828338436030943\n",
      "loss at 89 : 3.8689015632095347\n",
      "loss at 90 : 3.7184574615089603\n",
      "loss at 91 : 3.581892132838059\n",
      "loss at 92 : 3.676944429786023\n",
      "loss at 93 : 3.6714297889173677\n",
      "loss at 94 : 3.65213152886493\n",
      "loss at 95 : 3.6238522081483535\n",
      "loss at 96 : 3.672295994362588\n",
      "loss at 97 : 3.4994268604051415\n",
      "loss at 98 : 3.5091366495942493\n",
      "loss at 99 : 4.331383004744861\n"
     ]
    }
   ],
   "source": [
    "iterations = 100\n",
    "for i in range(iterations):\n",
    "    # Forward pass\n",
    "    o1 = b_affine(x, W[0])\n",
    "    o2 = sigmoid(o1)\n",
    "    o3 = b_affine(o2, W[1])\n",
    "    loss = softmax_ce(o3, y)\n",
    "    print(\"loss at {} : {}\".format(i, loss))\n",
    "    # Backward pass\n",
    "    d = softmax_ce_(o3, y)\n",
    "    d = b_affine_(d, o2, W[1])\n",
    "    dw1 = d['w']\n",
    "    d = d['x']\n",
    "    d = sigmoid_(d, o1)\n",
    "    d = b_affine_(d, x, W[0])\n",
    "    dw0 = d['w']\n",
    "    W[0] -= dw0 * 1e2\n",
    "    W[1] -= dw1 * 1e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 9 4 1 7 4 3 9 6] [5 0 4 1 9 2 1 3 1 4]\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(o3, axis=1)[:10], y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple binary NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<input>:72: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:93: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:72: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:93: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:72: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:93: DeprecationWarning: invalid escape sequence \\s\n",
      "<ipython-input-22-b7187a8755d0>:72: DeprecationWarning: invalid escape sequence \\s\n",
      "  \"\"\"\n",
      "<ipython-input-22-b7187a8755d0>:93: DeprecationWarning: invalid escape sequence \\s\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Weights must be of the format (in, hidden)\n",
    "shapes = [3, 1, 3]\n",
    "W = [np.random.randn(shapes[i], shapes[i+1]) for i in range(len(shapes)-1)]\n",
    "alpha = [np.mean(np.absolute(w)) for w in W]\n",
    "bW = [np.sign(w).astype(np.int8) for w in W]\n",
    "\n",
    "def l1(w):\n",
    "    return np.mean(np.absolute(w))\n",
    "\n",
    "def b(w):\n",
    "    return l1(w) * np.sign(w)\n",
    "\n",
    "def b_affine(x, w):\n",
    "    \"\"\"\n",
    "    x : (n, m)\n",
    "    w : (m, k)\n",
    "    out : (n, k)\n",
    "    \"\"\"\n",
    "    return x.dot(b(w))\n",
    "\n",
    "def b_affine_(d, x, w):\n",
    "    \"\"\"\n",
    "    x : (n, m)\n",
    "    w : (m, k)\n",
    "    d : (n, k)\n",
    "    ---\n",
    "    dw : (m, k)\n",
    "    dx : (n, m)\n",
    "    WARNING: Untested!\n",
    "    \"\"\"\n",
    "    # dw_ is binarized w's gradients\n",
    "    # dw is real gradients\n",
    "    dw_ = x.T.dot(d)\n",
    "    signw = np.sign(w)\n",
    "    n = w.size\n",
    "    # Multiplication rule: l1(w) / n * d[sign(w_i)]/dw_i\n",
    "    # dw = ((-1 <= w) * (w <= 1) * w) * l1(w) / n\n",
    "    # Multiplication rule: \\sum_j d[l1(w)/n]/dw_i * sign(w_j)\n",
    "    # dw += np.sum(dw_ * signw) / n * signw\n",
    "    dw = dw_ * (1/n + ((-5 <= w) * (w <= 5) * w) * l1(w) / n)\n",
    "    return {'x': d.dot(b(w).T), 'w': dw}\n",
    "\n",
    "def b_sigmoid(x):\n",
    "    \"\"\"\n",
    "    x : (n, m)\n",
    "    out : (n, m)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "    return sp.special.expit(x)\n",
    "\n",
    "def b_sigmoid_(d, x):\n",
    "    \"\"\"\n",
    "    x : (n, m)\n",
    "    d : (n, m)\n",
    "    ---\n",
    "    dx : (n, m)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "    sigm = sp.special.expit(x)\n",
    "    return d * (sigm * (1-sigm))\n",
    "\n",
    "def softmax_ce(x, y):\n",
    "    \"\"\"\n",
    "    x : (n, m)\n",
    "    y : (n,)\n",
    "    out : () [scalar]\n",
    "    \n",
    "    Equation is 1/n * \\sum_i^n [ -log(e^x_{y_i} / \\sum_j e^x_j) ]\n",
    "    which is equivalently:\n",
    "        \n",
    "        1/n * \\sum_i^n log(\\sum_j e^x_j) - x_{y_i}\n",
    "    \"\"\"\n",
    "    n = x.shape[0]\n",
    "    raise NotImplementedError\n",
    "    exp = np.exp(x)\n",
    "    # denominator in original expression, after log\n",
    "    denom = np.log(np.sum(exp, axis=1))\n",
    "    return np.sum(denom - x[np.arange(n), y]) / n\n",
    "\n",
    "def softmax_ce_(x, y):\n",
    "    \"\"\"\n",
    "    x : (n, m)\n",
    "    y : (n,)\n",
    "    ---\n",
    "    dx : (n, m)\n",
    "    \n",
    "    Back propagation for a single data point is:\n",
    "    \n",
    "    dL_i/dx_{ik} = -1_{k == y_i} + softmax(x_i)_k\n",
    "    \n",
    "    thus, for all data points:\n",
    "    dL/dx_k = 1/n * \\sum_i -1_{k == y_i} + softmax(x_i)_k\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "    n = x.shape[0]\n",
    "    exp = np.exp(x)\n",
    "    softmax = exp / np.expand_dims(np.sum(exp, axis=1), 1)\n",
    "    softmax[np.arange(n), y] -= 1\n",
    "    return softmax / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[-1,2,3]])\n",
    "print(W[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(W[0] * np.sign(W[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(alpha[0] * bW[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b_affine(x, W[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = b_affine_(np.ones((1,)), x, W[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(W[0], d['w'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = W[0] + d['w']\n",
    "print(w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b_affine(x, w0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affine_(np.ones((1,)), x, W[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_affine_(np.ones((1,)), x, W[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
