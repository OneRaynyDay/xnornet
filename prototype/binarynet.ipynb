{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn import datasets\n",
    "import mxnet as mx\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import functional as F\n",
    "mnist = mx.test_utils.get_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = mnist['train_data']\n",
    "x = x.reshape(x.shape[0], -1)\n",
    "y = mnist['train_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(y):\n",
    "    plt.plot(y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<input>:92: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:112: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:92: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:112: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:92: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:112: DeprecationWarning: invalid escape sequence \\s\n",
      "<ipython-input-46-a3761dbcb155>:92: DeprecationWarning: invalid escape sequence \\s\n",
      "  \"\"\"\n",
      "<ipython-input-46-a3761dbcb155>:112: DeprecationWarning: invalid escape sequence \\s\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Weights must be of the format (in, hidden)\n",
    "shapes = [784, 10, 10]\n",
    "\n",
    "### AFFINE ###\n",
    "def l1(w):\n",
    "    return np.mean(np.absolute(w))\n",
    "\n",
    "def b(w):\n",
    "    n = w.size\n",
    "    return l1(w) / n * np.sign(w)\n",
    "\n",
    "def b_affine(x, w):\n",
    "    \"\"\"\n",
    "    x : (n, m)\n",
    "    w : (m, k)\n",
    "    out : (n, k)\n",
    "    \"\"\"\n",
    "    return x.dot(b(w))\n",
    "##########\n",
    "\n",
    "def affine(x, w):\n",
    "    \"\"\"\n",
    "    x : (n, m)\n",
    "    w : (m, k)\n",
    "    out : (n, k)\n",
    "    \"\"\"\n",
    "    return x.dot(w)\n",
    "\n",
    "def affine_(d, x, w):\n",
    "    \"\"\"\n",
    "    x : (n, m)\n",
    "    w : (m, k)\n",
    "    d : (n, k)\n",
    "    ---\n",
    "    dw : (m, k)\n",
    "    dx : (n, m)\n",
    "    \"\"\"\n",
    "    return {'x': d.dot(w.T), 'w': x.T.dot(d)}\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    x : (n, m)\n",
    "    out : (n, m)\n",
    "    \"\"\"\n",
    "    return sp.special.expit(x)\n",
    "\n",
    "def sigmoid_(d, x):\n",
    "    \"\"\"\n",
    "    x : (n, m)\n",
    "    d : (n, m)\n",
    "    ---\n",
    "    dx : (n, m)\n",
    "    \"\"\"\n",
    "    sigm = sp.special.expit(x)\n",
    "    return d * (sigm * (1-sigm))\n",
    "\n",
    "def softmax_ce(x, y):\n",
    "    \"\"\"\n",
    "    x : (n, m)\n",
    "    y : (n,)\n",
    "    out : () [scalar]\n",
    "    \n",
    "    Equation is 1/n * \\sum_i^n [ -log(e^x_{y_i} / \\sum_j e^x_j) ]\n",
    "    which is equivalently:\n",
    "        \n",
    "        1/n * \\sum_i^n log(\\sum_j e^x_j) - x_{y_i}\n",
    "    \"\"\"\n",
    "    n = x.shape[0]\n",
    "    exp = np.exp(x)\n",
    "    # denominator in original expression, after log\n",
    "    denom = np.log(np.sum(exp, axis=1))\n",
    "    return np.sum(denom - x[np.arange(n), y]) / n\n",
    "\n",
    "def softmax_ce_(x, y):\n",
    "    \"\"\"\n",
    "    x : (n, m)\n",
    "    y : (n,)\n",
    "    ---\n",
    "    dx : (n, m)\n",
    "    \n",
    "    Back propagation for a single data point is:\n",
    "    \n",
    "    dL_i/dx_{ik} = -1_{k == y_i} + softmax(x_i)_k\n",
    "    \n",
    "    thus, for all data points:\n",
    "    dL/dx_k = 1/n * \\sum_i -1_{k == y_i} + softmax(x_i)_k\n",
    "    \"\"\"\n",
    "    n = x.shape[0]\n",
    "    exp = np.exp(x)\n",
    "    softmax = exp / np.expand_dims(np.sum(exp, axis=1), 1)\n",
    "    softmax[np.arange(n), y] -= 1\n",
    "    return softmax / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at 0 : 2.3028296174658704\n",
      "87.5858610673968\n",
      "loss at 1 : 1.916419321427279\n",
      "10407.399019983444\n",
      "loss at 2 : 1.6468456915070915\n",
      "18929.48025128426\n",
      "loss at 3 : 1.453807945919955\n",
      "26581.63486333422\n",
      "loss at 4 : 1.310009563649874\n",
      "33129.38398732028\n",
      "loss at 5 : 1.2127134149039926\n",
      "38991.216151467575\n",
      "loss at 6 : 1.135768281960623\n",
      "44103.73122633189\n",
      "loss at 7 : 1.1090247387896608\n",
      "48830.474984468616\n",
      "loss at 8 : 1.025589298552166\n",
      "53071.221306067746\n",
      "loss at 9 : 1.0000645950421654\n",
      "56922.30346321452\n",
      "loss at 10 : 0.9523862182930388\n",
      "60591.8810596474\n",
      "loss at 11 : 0.9598377364703511\n",
      "63925.945377298835\n",
      "loss at 12 : 0.8910327144828444\n",
      "67114.44689900786\n",
      "loss at 13 : 0.8991222973771152\n",
      "70006.88253109856\n",
      "loss at 14 : 0.8186131250486954\n",
      "72811.67277321359\n",
      "loss at 15 : 0.8060569907099839\n",
      "75284.05289059832\n",
      "loss at 16 : 0.7761361948084753\n",
      "77823.21443224521\n",
      "loss at 17 : 0.7722921429432621\n",
      "80088.08219371381\n",
      "loss at 18 : 0.7368274002080816\n",
      "82343.63229681825\n",
      "loss at 19 : 0.7179419339742665\n",
      "84328.51133317953\n",
      "loss at 20 : 0.7039130304254586\n",
      "86411.24919816696\n",
      "loss at 21 : 0.6920772759488864\n",
      "88317.90017580391\n",
      "loss at 22 : 0.6856971953025037\n",
      "90236.69877765792\n",
      "loss at 23 : 0.6843483205427607\n",
      "92055.98064826928\n",
      "loss at 24 : 0.6910505176405908\n",
      "93993.15687715507\n",
      "loss at 25 : 0.6805055077717749\n",
      "95676.90717614655\n",
      "loss at 26 : 0.6618174135548209\n",
      "97546.13266080437\n",
      "loss at 27 : 0.6613657729120838\n",
      "99077.64129309403\n",
      "loss at 28 : 0.6638480120740998\n",
      "100771.717826906\n",
      "loss at 29 : 0.6742690419988381\n",
      "102401.60708487504\n",
      "loss at 30 : 0.7223959680463647\n",
      "104069.27445920151\n",
      "loss at 31 : 0.64747038271843\n",
      "105778.13812938104\n",
      "loss at 32 : 0.6783479211687264\n",
      "107199.83218871623\n",
      "loss at 33 : 0.6873136742765991\n",
      "108962.01945280463\n",
      "loss at 34 : 0.7807613391215702\n",
      "110536.72787253099\n",
      "loss at 35 : 0.6362473376795726\n",
      "112274.51705427775\n",
      "loss at 36 : 0.6389526628593393\n",
      "113662.69913157733\n",
      "loss at 37 : 0.6053613886446558\n",
      "114999.35140797787\n",
      "loss at 38 : 0.6102877405972931\n",
      "116269.56091259225\n",
      "loss at 39 : 0.6046572185058584\n",
      "117617.8700021081\n",
      "loss at 40 : 0.6057570394948719\n",
      "118872.21451832222\n",
      "loss at 41 : 0.6300081003969192\n",
      "120298.63052741306\n",
      "loss at 42 : 0.7308321784799454\n",
      "121480.3944895853\n",
      "loss at 43 : 0.6792012492298163\n",
      "123187.46893999267\n",
      "loss at 44 : 0.7655865654014492\n",
      "124414.41201325756\n",
      "loss at 45 : 0.601877745998864\n",
      "126081.63356488038\n",
      "loss at 46 : 0.6240114965406262\n",
      "126980.23631975322\n",
      "loss at 47 : 0.6282020124275732\n",
      "128489.26528205375\n",
      "loss at 48 : 0.7083107604818089\n",
      "129480.90548102751\n",
      "loss at 49 : 0.5979825081989356\n",
      "131115.51808584275\n",
      "loss at 50 : 0.6011871494982445\n",
      "131993.9263096294\n",
      "loss at 51 : 0.580449297621076\n",
      "133316.11898015437\n",
      "loss at 52 : 0.6288886818021766\n",
      "134217.63918802174\n",
      "loss at 53 : 0.6496511692926963\n",
      "135561.11307238453\n",
      "loss at 54 : 0.7329862803493007\n",
      "136655.96474331\n",
      "loss at 55 : 0.6011846118780517\n",
      "138139.11363664034\n",
      "loss at 56 : 0.5644994690729653\n",
      "139036.7141992863\n",
      "loss at 57 : 0.5547373333764579\n",
      "140118.56136581406\n",
      "loss at 58 : 0.5827807229093214\n",
      "141008.30143943153\n",
      "loss at 59 : 0.6225664795887864\n",
      "142123.97904347148\n",
      "loss at 60 : 0.6407420217406299\n",
      "143158.20853489102\n",
      "loss at 61 : 0.6049275447091561\n",
      "144461.37696036216\n",
      "loss at 62 : 0.7502127732775163\n",
      "145284.50495201157\n",
      "loss at 63 : 0.7022800037082404\n",
      "146915.72979388534\n",
      "loss at 64 : 0.7290645881629404\n",
      "148011.93862268678\n",
      "loss at 65 : 0.6853365669088656\n",
      "149457.58153943677\n",
      "loss at 66 : 0.7473604898146926\n",
      "150268.53108567564\n",
      "loss at 67 : 0.6208627099299141\n",
      "151629.7974596788\n",
      "loss at 68 : 0.5907685925584097\n",
      "152669.91144643887\n",
      "loss at 69 : 0.5188596489324355\n",
      "153488.90560167958\n",
      "loss at 70 : 0.5276358575007505\n",
      "154308.67511564054\n",
      "loss at 71 : 0.5250637152253642\n",
      "155316.3386145123\n",
      "loss at 72 : 0.5745242516664364\n",
      "156007.3363123794\n",
      "loss at 73 : 0.609319342847995\n",
      "157057.46210209673\n",
      "loss at 74 : 0.6978345810952818\n",
      "157932.24681623036\n",
      "loss at 75 : 0.6134209121926931\n",
      "159339.9366804328\n",
      "loss at 76 : 0.6653451977818517\n",
      "160035.29323726697\n",
      "loss at 77 : 0.6092244466621416\n",
      "161377.7534373134\n",
      "loss at 78 : 0.6476762952535147\n",
      "162190.049414049\n",
      "loss at 79 : 0.6102042733812834\n",
      "163425.52067590013\n",
      "loss at 80 : 0.6529422000869819\n",
      "164068.08606791924\n",
      "loss at 81 : 0.6294099572300406\n",
      "165396.3901594508\n",
      "loss at 82 : 0.7386497160097847\n",
      "166229.8746485202\n",
      "loss at 83 : 0.684207651608979\n",
      "167545.74664670642\n",
      "loss at 84 : 0.7262352598476194\n",
      "168182.71030365137\n",
      "loss at 85 : 0.635252278507178\n",
      "169505.08713058307\n",
      "loss at 86 : 0.5607842740165209\n",
      "170429.73793673556\n",
      "loss at 87 : 0.5126164638962967\n",
      "171297.16084215834\n",
      "loss at 88 : 0.5131316755860887\n",
      "171939.79064990242\n",
      "loss at 89 : 0.592505355793132\n",
      "172793.64880637778\n",
      "loss at 90 : 0.6138977667873861\n",
      "173506.28745359962\n",
      "loss at 91 : 0.6679360423732505\n",
      "174681.26756419917\n",
      "loss at 92 : 0.6260004778637482\n",
      "175355.03981709384\n",
      "loss at 93 : 0.6984976193609327\n",
      "176559.5575343442\n",
      "loss at 94 : 0.696652373400058\n",
      "177255.00568753012\n",
      "loss at 95 : 0.7643491953201714\n",
      "178904.51683551684\n",
      "loss at 96 : 0.7768196543958474\n",
      "179488.26731450844\n",
      "loss at 97 : 0.7003779665891625\n",
      "181221.25512556414\n",
      "loss at 98 : 0.680946381799282\n",
      "182107.83806235873\n",
      "loss at 99 : 0.6982727810771444\n",
      "183121.93089683232\n"
     ]
    }
   ],
   "source": [
    "iterations = 100\n",
    "ALPHA = 1e4\n",
    "W = [np.random.randn(shapes[i], shapes[i+1]) for i in range(len(shapes)-1)]\n",
    "\n",
    "def b_affine_(d, x, w):\n",
    "    \"\"\"\n",
    "    x : (n, m)\n",
    "    w : (m, k)\n",
    "    d : (n, k)\n",
    "    ---\n",
    "    dw : (m, k)\n",
    "    dx : (n, m)\n",
    "    WARNING: Untested!\n",
    "    \"\"\"\n",
    "    # dw_ is binarized w's gradients\n",
    "    # dw is real gradients\n",
    "    dw_ = x.T.dot(d)\n",
    "    signw = np.sign(w)\n",
    "    n = w.size\n",
    "    dw = dw_ # * l1(w) / n * (-1 <= w) * (w <= 1)\n",
    "    print(np.linalg.norm(w))\n",
    "    # dw = dw_\n",
    "    # print(dw)\n",
    "    # Multiplication rule: \\sum_j d[l1(w)/n]/dw_i * sign(w_j)\n",
    "    # dw += np.sum(dw_ * signw) / n * signw\n",
    "    # dw += 1/n * dw_\n",
    "    return {'x': d.dot(b(w).T), 'w': dw}\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Forward pass\n",
    "    o1 = b_affine(x, W[0])\n",
    "    # o2 = sigmoid(o1)\n",
    "    # o3 = b_affine(o2, W[1])\n",
    "    loss = softmax_ce(o1, y) #change to o3 later\n",
    "    print(\"loss at {} : {}\".format(i, loss))\n",
    "    # Backward pass\n",
    "    d = softmax_ce_(o1, y)\n",
    "    # d = b_affine_(d, o2, W[1])\n",
    "    # dw1 = d['w']\n",
    "    # d = sigmoid_(d['x'], o1)\n",
    "    d = b_affine_(d, x, W[0])\n",
    "    dw0 = d['w']\n",
    "    W[0] -= dw0 * ALPHA\n",
    "    # W[1] -= dw1 * ALPHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8580833333333333\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(np.argmax(o1, axis=1) == y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple binary NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights must be of the format (in, hidden)\n",
    "shapes = [3, 1, 3]\n",
    "W = [np.random.randn(shapes[i], shapes[i+1]) for i in range(len(shapes)-1)]\n",
    "alpha = [np.mean(np.absolute(w)) for w in W]\n",
    "bW = [np.sign(w).astype(np.int8) for w in W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[-1,2,3]])\n",
    "print(W[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(W[0] * np.sign(W[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(alpha[0] * bW[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b_affine(x, W[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = b_affine_(np.ones((1,1)), x, W[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(W[0], d['w'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = W[0] + d['w']\n",
    "print(w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b_affine(x, w0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affine_(np.ones((1,1)), x, W[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_affine_(np.ones((1,1)), x, W[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
