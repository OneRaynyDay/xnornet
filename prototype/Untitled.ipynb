{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn import functional\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Binarize(torch.autograd.Function):\n",
    "    THRESHOLD_STE = True\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        We approximate the input by the following:\n",
    "        \n",
    "        input ~= sign(input) * l1_norm(input) / input.size\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.sign() * torch.mean(torch.abs(input))\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        According to [Do-Re-Fa Networks](https://arxiv.org/pdf/1606.06160.pdf),\n",
    "        the STE for binary weight networks is completely pass through.\n",
    "        \n",
    "        However, according to [Binary Neural Networks](https://arxiv.org/pdf/1602.02830.pdf),\n",
    "        and [XNOR-net networks](https://arxiv.org/pdf/1603.05279.pdf),\n",
    "        the STE must be thresholded by the following:\n",
    "        \n",
    "        d = d * (-1 <= w <= 1)\n",
    "        \n",
    "        Set THRESHOLD_STE to True/False for either behavior. However, it is suggested\n",
    "        to set it to True because we have seen performance degradations with it = False.\n",
    "        \"\"\"\n",
    "        if Binarize.THRESHOLD_STE:\n",
    "            input, = ctx.saved_tensors\n",
    "            grad_output[input.ge(1)] = 0\n",
    "            grad_output[input.le(-1)] = 0\n",
    "        return grad_output\n",
    "    \n",
    "class BinaryLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        \"\"\"\n",
    "        Takes in some inputs x, and initializes some weights for matmul,\n",
    "        and performs a bitcount(xor(x, weights)).\n",
    "        \n",
    "        input = (N, M)\n",
    "        weights = (M, K)\n",
    "        \n",
    "        in_features: size of each input sample\n",
    "        out_features: size of each output sample\n",
    "        bias: If set to False, the layer will not learn an additive bias.\n",
    "            Default: ``True``\n",
    "        \"\"\"\n",
    "        super(BinaryLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.bias = torch.nn.Parameter(torch.Tensor(out_features))\n",
    "        \n",
    "        # Initializing parameters\n",
    "        stdv = 1. / math.sqrt(in_features * out_features)\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)        \n",
    "\n",
    "    def forward(self, input):\n",
    "        binarize = Binarize.apply\n",
    "        return functional.linear(binarize(input), binarize(self.weight), binarize(self.bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmul = BinaryLinear(3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7516, -1.9929,  0.2617]]) \n",
      " Parameter containing:\n",
      "tensor([ 0.5027]) \n",
      " Parameter containing:\n",
      "tensor([[ 0.4418,  0.0777, -0.5062]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5252]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 3, requires_grad=True)\n",
    "print(x, '\\n', bmul.bias, '\\n', bmul.weight)\n",
    "bmul.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proof of correctness : linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> import numpy as np\n",
    "# >>> x = np.array([[-0.7516, -1.9929,  0.2617]])\n",
    "# >>> w = np.array([[ 0.4418,  0.0777, -0.5062]])\n",
    "# >>> ax = np.mean(np.absolute(x))\n",
    "# >>> ax\n",
    "# 1.0020666666666667\n",
    "# >>> aw = np.mean(np.absolute(w))\n",
    "# >>> aw\n",
    "# 0.34190000000000004\n",
    "# >>> Bx = np.sign(x)\n",
    "# >>> Bw = np.sign(w)\n",
    "# >>> print(\"w:\",w,\"bw:\",Bw * aw)\n",
    "# w: [[ 0.4418  0.0777 -0.5062]] bw: [[ 0.3419  0.3419 -0.3419]]\n",
    "# >>> linear = np.sum(Bx * Bw)\n",
    "# >>> linear *= ax * aw\n",
    "# >>> linear\n",
    "# -1.0278197800000002\n",
    "# >>> linear += b\n",
    "# >>> linear\n",
    "# -0.5251197800000001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proof of correctness : convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
