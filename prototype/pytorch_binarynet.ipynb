{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn import functional\n",
    "import math\n",
    "\n",
    "# Hyper-parameters \n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "num_epochs = 1\n",
    "batch_size = 32\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# MNIST dataset (images and labels)\n",
    "train_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader (input pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Binarize(torch.autograd.Function):\n",
    "    THRESHOLD_STE = True\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        We approximate the input by the following:\n",
    "        \n",
    "        input ~= sign(input) * l1_norm(input) / input.size\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.sign() * torch.mean(torch.abs(input))\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        According to [Do-Re-Fa Networks](https://arxiv.org/pdf/1606.06160.pdf),\n",
    "        the STE for binary weight networks is completely pass through.\n",
    "        \n",
    "        However, according to [Binary Neural Networks](https://arxiv.org/pdf/1602.02830.pdf),\n",
    "        and [XNOR-net networks](https://arxiv.org/pdf/1603.05279.pdf),\n",
    "        the STE must be thresholded by the following:\n",
    "        \n",
    "        d = d * (-1 <= w <= 1)\n",
    "        \n",
    "        Set THRESHOLD_STE to True/False for either behavior. However, it is suggested\n",
    "        to set it to True because we have seen performance degradations with it = False.\n",
    "        \"\"\"\n",
    "        if Binarize.THRESHOLD_STE:\n",
    "            input, = ctx.saved_tensors\n",
    "            grad_output[input.ge(1)] = 0\n",
    "            grad_output[input.le(-1)] = 0\n",
    "        return grad_output\n",
    "\n",
    "class BinaryLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        \"\"\"\n",
    "        Takes in some inputs x, and initializes some weights for matmul,\n",
    "        and performs a bitcount(xor(x, weights)).\n",
    "        \n",
    "        input = (N, M)\n",
    "        weights = (M, K)\n",
    "        \n",
    "        in_features: size of each input sample\n",
    "        out_features: size of each output sample\n",
    "        bias: If set to False, the layer will not learn an additive bias.\n",
    "            Default: ``True``\n",
    "        \"\"\"\n",
    "        super(BinaryLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.bias = torch.nn.Parameter(torch.Tensor(out_features))\n",
    "        \n",
    "        # Initializing parameters\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)        \n",
    "\n",
    "    def forward(self, input):\n",
    "        binarize = Binarize.apply\n",
    "        return functional.linear(binarize(input), binarize(self.weight), binarize(self.bias))\n",
    "    \n",
    "class BinaryConvolution2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, bias=True):\n",
    "        \"\"\"\n",
    "        Takes in some inputs x, and initializes some weights for conv filters,\n",
    "        and performs a \"convolution\" by binarizing the weights and multiplying\n",
    "        the inputs by the binarized weights.\n",
    "        \n",
    "        input = (N, C, H, W)\n",
    "        weights = (K, C, H, W) [ to be binarized ]\n",
    "        biases = (K,) [ to be binarized ]\n",
    "        output = (N, K, H, W)\n",
    "        \n",
    "        in_channels (int): Number of channels in the input image\n",
    "        out_channels (int): Number of channels produced by the convolution\n",
    "        kernel_size (int): Size of the convolving kernel\n",
    "        stride (int or tuple, optional): Stride of the convolution. Default: 1\n",
    "        padding (int or tuple, optional): Zero-padding added to both sides of the input. Default: 0\n",
    "        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``\n",
    "        \n",
    "        NOTE: We skip dilation, groups, etc for now.\n",
    "        \"\"\"\n",
    "        super(BinaryConvolution2d, self).__init__()\n",
    "        self.weight = torch.nn.Parameter(torch.Tensor(out_channels, in_channels, *(kernel_size, kernel_size)))\n",
    "        self.bias = torch.nn.Parameter(torch.Tensor(out_channels))\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        # Initializing parameters\n",
    "        n = in_channels\n",
    "        n *= kernel_size ** 2 # number of parameters\n",
    "        stdv = 1. / math.sqrt(n)\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        self.bias.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        binarize = Binarize.apply\n",
    "        return functional.conv2d(binarize(input), binarize(self.weight), binarize(self.bias), self.stride, self.padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1193, -0.2836, -0.2486]])\n",
      "tensor([[-0.2172, -0.2172, -0.2172]])\n",
      "16.747344970703125\n",
      "tensor([[-2.4343, -4.4343, -6.4343]])\n",
      "---\n",
      "tensor([[ 0.1242,  0.1598,  0.3949]])\n",
      "tensor([[ 0.2263,  0.2263,  0.2263]])\n",
      "11.438299179077148\n",
      "tensor([[-1.5474, -3.5474, -5.5474]])\n",
      "---\n",
      "tensor([[ 0.2789,  0.5146,  0.9496]])\n",
      "tensor([[ 0.5810,  0.5810,  0.5810]])\n",
      "8.040512084960938\n",
      "tensor([[-0.8380, -2.8380, -4.8380]])\n",
      "---\n",
      "tensor([[ 0.3627,  0.7983,  1.4334]])\n",
      "tensor([[ 0.8648,  0.8648,  0.8648]])\n",
      "5.865927219390869\n",
      "tensor([[-0.2704, -2.2704,  0.0000]])\n",
      "---\n",
      "tensor([[ 0.3897,  1.0254,  1.4334]])\n",
      "tensor([[ 0.9495,  0.9495,  0.9495]])\n",
      "5.310605049133301\n",
      "tensor([[-0.1010,  0.0000,  0.0000]])\n",
      "---\n",
      "tensor([[ 0.3998,  1.0254,  1.4334]])\n",
      "tensor([[ 0.9529,  0.9529,  0.9529]])\n",
      "5.289422035217285\n",
      "tensor(1.00000e-02 *\n",
      "       [[-9.4253,  0.0000,  0.0000]])\n",
      "---\n",
      "tensor([[ 0.4093,  1.0254,  1.4334]])\n",
      "tensor([[ 0.9560,  0.9560,  0.9560]])\n",
      "5.269712924957275\n",
      "tensor(1.00000e-02 *\n",
      "       [[-8.7970,  0.0000,  0.0000]])\n",
      "---\n",
      "tensor([[ 0.4180,  1.0254,  1.4334]])\n",
      "tensor([[ 0.9589,  0.9589,  0.9589]])\n",
      "5.251369953155518\n",
      "tensor(1.00000e-02 *\n",
      "       [[-8.2105,  0.0000,  0.0000]])\n",
      "---\n",
      "tensor([[ 0.4263,  1.0254,  1.4334]])\n",
      "tensor([[ 0.9617,  0.9617,  0.9617]])\n",
      "5.2342987060546875\n",
      "tensor(1.00000e-02 *\n",
      "       [[-7.6631,  0.0000,  0.0000]])\n",
      "---\n",
      "tensor([[ 0.4339,  1.0254,  1.4334]])\n",
      "tensor([[ 0.9642,  0.9642,  0.9642]])\n",
      "5.218404293060303\n",
      "tensor(1.00000e-02 *\n",
      "       [[-7.1522,  0.0000,  0.0000]])\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Testing Binarize:\n",
    "# Create random Tensors to hold input and outputs.\n",
    "x = torch.randn(1, 3, requires_grad=True)\n",
    "\n",
    "b = Binarize.apply\n",
    "\n",
    "# Forward pass: compute predicted y using operations; we compute\n",
    "# ReLU using our custom autograd operation.\n",
    "for _ in range(10):\n",
    "    y = b(x)\n",
    "    loss = (y - torch.FloatTensor([1,2,3])).pow(2).sum()\n",
    "    loss.backward()\n",
    "\n",
    "    print(x)\n",
    "    print(y)\n",
    "    print(loss.item())\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        x -= x.grad * 1e-1\n",
    "        print(x.grad)\n",
    "        x.grad.zero_()\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            BinaryConvolution2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            # nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.BatchNorm2d(16),\n",
    "            BinaryConvolution2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            # nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "        )\n",
    "        # self.fc = nn.Linear(7*7*32, num_classes) \n",
    "        self.fc = BinaryLinear(7*7*32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        # Flatten\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = ConvNet(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [10/1875], Loss: 1.9302\n",
      "Epoch [1/1], Step [20/1875], Loss: 0.0564\n",
      "Epoch [1/1], Step [30/1875], Loss: 0.8536\n",
      "Epoch [1/1], Step [40/1875], Loss: 0.9208\n",
      "Epoch [1/1], Step [50/1875], Loss: 0.3538\n",
      "Epoch [1/1], Step [60/1875], Loss: 1.0371\n",
      "Epoch [1/1], Step [70/1875], Loss: 0.8205\n",
      "Epoch [1/1], Step [80/1875], Loss: 1.8055\n",
      "Epoch [1/1], Step [90/1875], Loss: 0.2843\n",
      "Epoch [1/1], Step [100/1875], Loss: 0.1688\n",
      "Epoch [1/1], Step [110/1875], Loss: 0.3524\n",
      "Epoch [1/1], Step [120/1875], Loss: 0.0183\n",
      "Epoch [1/1], Step [130/1875], Loss: 1.7730\n",
      "Epoch [1/1], Step [140/1875], Loss: 0.5387\n",
      "Epoch [1/1], Step [150/1875], Loss: 1.1975\n",
      "Epoch [1/1], Step [160/1875], Loss: 0.5156\n",
      "Epoch [1/1], Step [170/1875], Loss: 0.2278\n",
      "Epoch [1/1], Step [180/1875], Loss: 0.8207\n",
      "Epoch [1/1], Step [190/1875], Loss: 0.0156\n",
      "Epoch [1/1], Step [200/1875], Loss: 1.1427\n",
      "Epoch [1/1], Step [210/1875], Loss: 0.2397\n",
      "Epoch [1/1], Step [220/1875], Loss: 0.6977\n",
      "Epoch [1/1], Step [230/1875], Loss: 1.0472\n",
      "Epoch [1/1], Step [240/1875], Loss: 0.0004\n",
      "Epoch [1/1], Step [250/1875], Loss: 0.1576\n",
      "Epoch [1/1], Step [260/1875], Loss: 1.2928\n",
      "Epoch [1/1], Step [270/1875], Loss: 1.5164\n",
      "Epoch [1/1], Step [280/1875], Loss: 1.5476\n",
      "Epoch [1/1], Step [290/1875], Loss: 0.5987\n",
      "Epoch [1/1], Step [300/1875], Loss: 0.1681\n",
      "Epoch [1/1], Step [310/1875], Loss: 0.6310\n",
      "Epoch [1/1], Step [320/1875], Loss: 0.2804\n",
      "Epoch [1/1], Step [330/1875], Loss: 1.9365\n",
      "Epoch [1/1], Step [340/1875], Loss: 1.0039\n",
      "Epoch [1/1], Step [350/1875], Loss: 0.0624\n",
      "Epoch [1/1], Step [360/1875], Loss: 0.4810\n",
      "Epoch [1/1], Step [370/1875], Loss: 1.7855\n",
      "Epoch [1/1], Step [380/1875], Loss: 1.1882\n",
      "Epoch [1/1], Step [390/1875], Loss: 1.5262\n",
      "Epoch [1/1], Step [400/1875], Loss: 0.8598\n",
      "Epoch [1/1], Step [410/1875], Loss: 0.0001\n",
      "Epoch [1/1], Step [420/1875], Loss: 0.4378\n",
      "Epoch [1/1], Step [430/1875], Loss: 0.4605\n",
      "Epoch [1/1], Step [440/1875], Loss: 1.7076\n",
      "Epoch [1/1], Step [450/1875], Loss: 0.2461\n",
      "Epoch [1/1], Step [460/1875], Loss: 3.5400\n",
      "Epoch [1/1], Step [470/1875], Loss: 1.3737\n",
      "Epoch [1/1], Step [480/1875], Loss: 0.6573\n",
      "Epoch [1/1], Step [490/1875], Loss: 0.7648\n",
      "Epoch [1/1], Step [500/1875], Loss: 1.1629\n",
      "Epoch [1/1], Step [510/1875], Loss: 1.0533\n",
      "Epoch [1/1], Step [520/1875], Loss: 0.3967\n",
      "Epoch [1/1], Step [530/1875], Loss: 0.3681\n",
      "Epoch [1/1], Step [540/1875], Loss: 1.6963\n",
      "Epoch [1/1], Step [550/1875], Loss: 1.1921\n",
      "Epoch [1/1], Step [560/1875], Loss: 0.6214\n",
      "Epoch [1/1], Step [570/1875], Loss: 0.3299\n",
      "Epoch [1/1], Step [580/1875], Loss: 0.8156\n",
      "Epoch [1/1], Step [590/1875], Loss: 0.5893\n",
      "Epoch [1/1], Step [600/1875], Loss: 0.0293\n",
      "Epoch [1/1], Step [610/1875], Loss: 2.3097\n",
      "Epoch [1/1], Step [620/1875], Loss: 1.3231\n",
      "Epoch [1/1], Step [630/1875], Loss: 0.0028\n",
      "Epoch [1/1], Step [640/1875], Loss: 0.0002\n",
      "Epoch [1/1], Step [650/1875], Loss: 0.0009\n",
      "Epoch [1/1], Step [660/1875], Loss: 0.9434\n",
      "Epoch [1/1], Step [670/1875], Loss: 0.0506\n",
      "Epoch [1/1], Step [680/1875], Loss: 0.0002\n",
      "Epoch [1/1], Step [690/1875], Loss: 0.1478\n",
      "Epoch [1/1], Step [700/1875], Loss: 0.8416\n",
      "Epoch [1/1], Step [710/1875], Loss: 1.1957\n",
      "Epoch [1/1], Step [720/1875], Loss: 0.1413\n",
      "Epoch [1/1], Step [730/1875], Loss: 0.9270\n",
      "Epoch [1/1], Step [740/1875], Loss: 1.1939\n",
      "Epoch [1/1], Step [750/1875], Loss: 0.5043\n",
      "Epoch [1/1], Step [760/1875], Loss: 0.5811\n",
      "Epoch [1/1], Step [770/1875], Loss: 1.1298\n",
      "Epoch [1/1], Step [780/1875], Loss: 1.0270\n",
      "Epoch [1/1], Step [790/1875], Loss: 1.0639\n",
      "Epoch [1/1], Step [800/1875], Loss: 1.9999\n",
      "Epoch [1/1], Step [810/1875], Loss: 0.4183\n",
      "Epoch [1/1], Step [820/1875], Loss: 0.8708\n",
      "Epoch [1/1], Step [830/1875], Loss: 0.8706\n",
      "Epoch [1/1], Step [840/1875], Loss: 0.3703\n",
      "Epoch [1/1], Step [850/1875], Loss: 0.1796\n",
      "Epoch [1/1], Step [860/1875], Loss: 0.1090\n",
      "Epoch [1/1], Step [870/1875], Loss: 0.0286\n",
      "Epoch [1/1], Step [880/1875], Loss: 0.0000\n",
      "Epoch [1/1], Step [890/1875], Loss: 0.5310\n",
      "Epoch [1/1], Step [900/1875], Loss: 0.0871\n",
      "Epoch [1/1], Step [910/1875], Loss: 1.0592\n",
      "Epoch [1/1], Step [920/1875], Loss: 0.2254\n",
      "Epoch [1/1], Step [930/1875], Loss: 0.1957\n",
      "Epoch [1/1], Step [940/1875], Loss: 0.3278\n",
      "Epoch [1/1], Step [950/1875], Loss: 1.0316\n",
      "Epoch [1/1], Step [960/1875], Loss: 0.0048\n",
      "Epoch [1/1], Step [970/1875], Loss: 0.0030\n",
      "Epoch [1/1], Step [980/1875], Loss: 0.9296\n",
      "Epoch [1/1], Step [990/1875], Loss: 1.1208\n",
      "Epoch [1/1], Step [1000/1875], Loss: 1.5021\n",
      "Epoch [1/1], Step [1010/1875], Loss: 0.0024\n",
      "Epoch [1/1], Step [1020/1875], Loss: 1.0917\n",
      "Epoch [1/1], Step [1030/1875], Loss: 0.0013\n",
      "Epoch [1/1], Step [1040/1875], Loss: 0.0266\n",
      "Epoch [1/1], Step [1050/1875], Loss: 1.5619\n",
      "Epoch [1/1], Step [1060/1875], Loss: 1.5400\n",
      "Epoch [1/1], Step [1070/1875], Loss: 0.1029\n",
      "Epoch [1/1], Step [1080/1875], Loss: 1.6964\n",
      "Epoch [1/1], Step [1090/1875], Loss: 1.2023\n",
      "Epoch [1/1], Step [1100/1875], Loss: 0.2483\n",
      "Epoch [1/1], Step [1110/1875], Loss: 0.1157\n",
      "Epoch [1/1], Step [1120/1875], Loss: 0.3054\n",
      "Epoch [1/1], Step [1130/1875], Loss: 1.5923\n",
      "Epoch [1/1], Step [1140/1875], Loss: 0.4651\n",
      "Epoch [1/1], Step [1150/1875], Loss: 0.2305\n",
      "Epoch [1/1], Step [1160/1875], Loss: 0.8204\n",
      "Epoch [1/1], Step [1170/1875], Loss: 0.5697\n",
      "Epoch [1/1], Step [1180/1875], Loss: 0.3285\n",
      "Epoch [1/1], Step [1190/1875], Loss: 0.6365\n",
      "Epoch [1/1], Step [1200/1875], Loss: 1.3580\n",
      "Epoch [1/1], Step [1210/1875], Loss: 1.7491\n",
      "Epoch [1/1], Step [1220/1875], Loss: 0.0000\n",
      "Epoch [1/1], Step [1230/1875], Loss: 0.3100\n",
      "Epoch [1/1], Step [1240/1875], Loss: 0.5055\n",
      "Epoch [1/1], Step [1250/1875], Loss: 0.5196\n",
      "Epoch [1/1], Step [1260/1875], Loss: 0.6438\n",
      "Epoch [1/1], Step [1270/1875], Loss: 0.7713\n",
      "Epoch [1/1], Step [1280/1875], Loss: 0.0018\n",
      "Epoch [1/1], Step [1290/1875], Loss: 0.0842\n",
      "Epoch [1/1], Step [1300/1875], Loss: 0.2674\n",
      "Epoch [1/1], Step [1310/1875], Loss: 0.6632\n",
      "Epoch [1/1], Step [1320/1875], Loss: 1.3787\n",
      "Epoch [1/1], Step [1330/1875], Loss: 1.6439\n",
      "Epoch [1/1], Step [1340/1875], Loss: 0.1018\n",
      "Epoch [1/1], Step [1350/1875], Loss: 0.5879\n",
      "Epoch [1/1], Step [1360/1875], Loss: 0.4844\n",
      "Epoch [1/1], Step [1370/1875], Loss: 0.0000\n",
      "Epoch [1/1], Step [1380/1875], Loss: 0.1143\n",
      "Epoch [1/1], Step [1390/1875], Loss: 1.0455\n",
      "Epoch [1/1], Step [1400/1875], Loss: 0.0964\n",
      "Epoch [1/1], Step [1410/1875], Loss: 0.2541\n",
      "Epoch [1/1], Step [1420/1875], Loss: 1.6182\n",
      "Epoch [1/1], Step [1430/1875], Loss: 0.4752\n",
      "Epoch [1/1], Step [1440/1875], Loss: 0.3719\n",
      "Epoch [1/1], Step [1450/1875], Loss: 1.2632\n",
      "Epoch [1/1], Step [1460/1875], Loss: 0.5242\n",
      "Epoch [1/1], Step [1470/1875], Loss: 0.3167\n",
      "Epoch [1/1], Step [1480/1875], Loss: 0.0440\n",
      "Epoch [1/1], Step [1490/1875], Loss: 0.4250\n",
      "Epoch [1/1], Step [1500/1875], Loss: 0.0425\n",
      "Epoch [1/1], Step [1510/1875], Loss: 0.8013\n",
      "Epoch [1/1], Step [1520/1875], Loss: 0.0086\n",
      "Epoch [1/1], Step [1530/1875], Loss: 0.8933\n",
      "Epoch [1/1], Step [1540/1875], Loss: 0.6133\n",
      "Epoch [1/1], Step [1550/1875], Loss: 0.3843\n",
      "Epoch [1/1], Step [1560/1875], Loss: 0.0432\n",
      "Epoch [1/1], Step [1570/1875], Loss: 0.1811\n",
      "Epoch [1/1], Step [1580/1875], Loss: 1.0214\n",
      "Epoch [1/1], Step [1590/1875], Loss: 0.7514\n",
      "Epoch [1/1], Step [1600/1875], Loss: 0.8789\n",
      "Epoch [1/1], Step [1610/1875], Loss: 0.0042\n",
      "Epoch [1/1], Step [1620/1875], Loss: 0.0941\n",
      "Epoch [1/1], Step [1630/1875], Loss: 1.3154\n",
      "Epoch [1/1], Step [1640/1875], Loss: 0.8162\n",
      "Epoch [1/1], Step [1650/1875], Loss: 0.2541\n",
      "Epoch [1/1], Step [1660/1875], Loss: 0.9372\n",
      "Epoch [1/1], Step [1670/1875], Loss: 0.4689\n",
      "Epoch [1/1], Step [1680/1875], Loss: 0.2223\n",
      "Epoch [1/1], Step [1690/1875], Loss: 0.1906\n",
      "Epoch [1/1], Step [1700/1875], Loss: 0.9867\n",
      "Epoch [1/1], Step [1710/1875], Loss: 1.7978\n",
      "Epoch [1/1], Step [1720/1875], Loss: 0.0102\n",
      "Epoch [1/1], Step [1730/1875], Loss: 0.0007\n",
      "Epoch [1/1], Step [1740/1875], Loss: 0.3856\n",
      "Epoch [1/1], Step [1750/1875], Loss: 0.4367\n",
      "Epoch [1/1], Step [1760/1875], Loss: 0.0018\n",
      "Epoch [1/1], Step [1770/1875], Loss: 0.1990\n",
      "Epoch [1/1], Step [1780/1875], Loss: 0.2309\n",
      "Epoch [1/1], Step [1790/1875], Loss: 1.0049\n",
      "Epoch [1/1], Step [1800/1875], Loss: 0.4682\n",
      "Epoch [1/1], Step [1810/1875], Loss: 1.2439\n",
      "Epoch [1/1], Step [1820/1875], Loss: 0.0001\n",
      "Epoch [1/1], Step [1830/1875], Loss: 0.0007\n",
      "Epoch [1/1], Step [1840/1875], Loss: 0.1225\n",
      "Epoch [1/1], Step [1850/1875], Loss: 0.9509\n",
      "Epoch [1/1], Step [1860/1875], Loss: 0.1465\n",
      "Epoch [1/1], Step [1870/1875], Loss: 0.4900\n"
     ]
    }
   ],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if (i+1) % 10 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 92.53 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): BinaryConvolution2d()\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): BinaryConvolution2d()\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (fc): BinaryLinear()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(repr(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c91dc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
